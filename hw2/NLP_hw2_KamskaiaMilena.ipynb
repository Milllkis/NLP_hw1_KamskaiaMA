{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашняя работа №2, сравнение теггеров\n",
        "\n",
        "Работу выполнила Камская Милена, БКЛ-211"
      ],
      "metadata": {
        "id": "3tPvo6UppDSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратим внимание на файл gold.conllu\n",
        "\n",
        "В нем лежат 24 размеченных предложения, 235 слов. Я использовала **Universal POS tags**, чтобы в дальнейшем не усложнять себе жизнь, т.к. он часто используется и в других POS теггерах (более того, я возьму именно те теггеры, в которых используется upos).\n",
        "\n",
        "Касательно сложных моментов:\n",
        "\n",
        "1. __Заимствования__\n",
        "        \n",
        "      Я брала те слова, что редко присутсвуют в повседневной речи и с большой вероятностью не подчиняются стандартным правилам русского язык.\n",
        "2. __Составные предлоги__\n",
        "\n",
        "      Теггер может их разделить и начать неверно трактовать. К примеру, _за счет_ это полноценный ADP, а не ADP+NOUN\n",
        "3. __Слова через дефис__\n",
        "\n",
        "      Я брала те слова, которые нельзя разделять по дефису. Сюда входят и предлоги (_из-за_), и наречия (_по-человечески_), и существительные (_пол-лимона_), и глаголы с частицами (_Подойди-ка_), и междометия (_ей-богу_)\n",
        "4. __Отглагольные предлоги__\n",
        "\n",
        "      Я взяла отглагольные предлоги (к примеру, _благодаря_), т.к. их можно спутать с деепричастиями"
      ],
      "metadata": {
        "id": "0eMKhE5EpCxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stanza"
      ],
      "metadata": {
        "id": "s68Q-7gDpfrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stanza"
      ],
      "metadata": {
        "id": "wzDSpLilp12V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "\n",
        "nlp_stanza = stanza.Pipeline(lang='ru', processors='tokenize,pos')"
      ],
      "metadata": {
        "id": "68gtc9K2ptpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_by_stanza(file):\n",
        "    with open('pos_by_stanza.txt', 'w', encoding='UTF-8') as output:\n",
        "        with open(file, 'r', encoding='UTF-8') as input:\n",
        "            f = input.readlines()\n",
        "            for line in f:\n",
        "                if 'text' in line:\n",
        "                    sentences = nlp_stanza(line[9:-1])\n",
        "                    for sent in sentences.sentences:\n",
        "                        for token in sent.words:\n",
        "                          output.write(token.text + '\\t' + token.upos + '\\n')"
      ],
      "metadata": {
        "id": "nY39hMGFgWZb"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_by_stanza('gold.conllu')"
      ],
      "metadata": {
        "id": "12ZNCjSFh3aW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Natasha"
      ],
      "metadata": {
        "id": "hyJeUCg6pmj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install natasha"
      ],
      "metadata": {
        "id": "dgWXoWtKqGzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    Doc\n",
        ")\n",
        "\n",
        "\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)"
      ],
      "metadata": {
        "id": "PnTKFpCMqDNU"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_by_natasha(file):\n",
        "    with open('pos_by_natasha.txt', 'w', encoding='UTF-8') as output:\n",
        "        with open(file, 'r', encoding='UTF-8') as input:\n",
        "            f = input.readlines()\n",
        "            for line in f:\n",
        "                if 'text' in line:\n",
        "                    sent = Doc(line[9:-1])\n",
        "                    sent.segment(segmenter)\n",
        "                    sent.tag_morph(morph_tagger)\n",
        "                    for token in sent.tokens:\n",
        "                        output.write(token.text + '\\t' + token.pos + '\\n')"
      ],
      "metadata": {
        "id": "IP5ffQyfejRF"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_by_natasha('gold.conllu')"
      ],
      "metadata": {
        "id": "eNAP2vC5fRSc"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy"
      ],
      "metadata": {
        "id": "_TJtBgGEpl4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "MKH_Cbe5Zvjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "1zK3a17Slz9R"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_spacy = spacy.load(\"ru_core_news_sm\")"
      ],
      "metadata": {
        "id": "l66LiNTZeKPd"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_by_spacy(file):\n",
        "    with open('pos_by_spacy.txt', 'w', encoding='UTF-8') as output:\n",
        "        with open(file, 'r', encoding='UTF-8') as input:\n",
        "            f = input.readlines()\n",
        "            for line in f:\n",
        "                if 'text' in line:\n",
        "                    sentence = nlp_spacy(line[9:-1])\n",
        "                    for token in sentence:\n",
        "                        output.write(token.text + '\\t' + token.pos_ + '\\n')"
      ],
      "metadata": {
        "id": "10kU18OxaZeL"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_by_spacy('gold.conllu')"
      ],
      "metadata": {
        "id": "PzQzJnVBd03w"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy Test (точнее мои грустные попытки)"
      ],
      "metadata": {
        "id": "a6sMr3UKiVI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from collections import defaultdict\n",
        "\n",
        "def accuracy_testing(file_gold, file_pos):\n",
        "    results = []\n",
        "    golden = []\n",
        "    d_gold = defaultdict(list)\n",
        "    d_pred = defaultdict(list)\n",
        "\n",
        "    with open(file_gold, 'r', encoding='UTF-8') as gold:\n",
        "        for line in gold.readlines():\n",
        "            if ('#' not in line) and (line[0] != '\\n'):\n",
        "                line = line.split('\\t')\n",
        "                d_gold[line[1]].append(line[3])\n",
        "                golden.append(line[3])\n",
        "\n",
        "    with open(file_pos, 'r', encoding='UTF-8') as pred:\n",
        "        for pred_line in pred.readlines():\n",
        "            pred_line = pred_line.strip('\\n')\n",
        "            pred_line = pred_line.split('\\t')\n",
        "            d_pred[pred_line[0]].append(pred_line[1])\n",
        "\n",
        "    for el, pos in d_gold.items():\n",
        "        if el in d_pred.keys():\n",
        "            results.append(d_pred[el])\n",
        "        else:\n",
        "            results.append('error')\n",
        "\n",
        "    print(\"Accuracy: %.4f\" % accuracy_score(golden,results))"
      ],
      "metadata": {
        "id": "hP-oe-FSOdGD"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_stanza = accuracy_testing('gold.conllu', 'pos_by_stanza.txt')\n",
        "res_spacy = accuracy_testing('gold.conllu', 'pos_by_spacy.txt')\n",
        "res_natasha = accuracy_testing('gold.conllu', 'pos_by_natasha.txt')"
      ],
      "metadata": {
        "id": "WkcNg7-N33ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я понимаю, что проблема в том, что я сравниваю 2 разноразмерных списка, но починить это я не в силах. Если вдруг есть какие-то комментарии к тому, как я могу это сделать, то было бы очень славно послушать. Если таких комментариев нет, надо просто забить и ловить дзен"
      ],
      "metadata": {
        "id": "rZwyuHNRlvix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_stanza"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdueVPOYok_c",
        "outputId": "06445d36-0347-4b08-c245-ac0eb548c937"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD3FgawKo2DO",
        "outputId": "39ff83f9-d2fd-4f3e-dd37-3c2c01c57637"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiAH-Xb7o4am",
        "outputId": "b24be19e-0d7d-4172-b1b8-bbf85a8887f9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunker"
      ],
      "metadata": {
        "id": "rH_UbIiup4r5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я буду делать chunker для stanza\n",
        "\n",
        "Я выделила 3 шаблона:\n",
        "1. __\"не\"+VERB__\n",
        "\n",
        "      Это сильно повлияет на результаты, т.к. рассматривать раздельно эти вещи нельзя. Именно частица _не_ играет огромную роль в изменении тональности (сравните: _понравился_ и _не понравился_)\n",
        "2. __ADV+VERB__\n",
        "\n",
        "      Аналогично прошлому пункту. (сравните: _хорошо смотрелся_ и _ужасно смотрелся_)\n",
        "3. __VERB+ADV__\n",
        "\n",
        "      Я, конечно, понимаю, что это очень похоже на прошлый шаблон, но, изучая отзывы, я нашла немалое количество примеров по типу _звучало ужасно_ и _звучало шикарно_, которые важны для более корректного определения тональности"
      ],
      "metadata": {
        "id": "c2RA5IXnp8YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним все нужные импорты"
      ],
      "metadata": {
        "id": "lM1MXGiEFDZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import csv\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2xCUhxJFF_5",
        "outputId": "fdc369d6-ba27-4384-def2-6c9a8f440929"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunker(text):\n",
        "    words_pos = []\n",
        "    for sentence in sent_tokenize(text):\n",
        "        stanza_app = nlp_stanza(sentence)\n",
        "        for sent in stanza_app.sentences:\n",
        "            for token in sent.words:\n",
        "                words_pos.append([token.text, token.upos])\n",
        "\n",
        "    bigrams = []\n",
        "    for i in range(0, (len(words_pos)-1)):\n",
        "        if words_pos[i][0] == 'не' and words_pos[i+1][1] == 'VERB':\n",
        "            bigram = words_pos[i][0]+' '+words_pos[i+1][0]\n",
        "            bigrams.append(bigram)\n",
        "\n",
        "        if words_pos[i][1] == 'ADV' and words_pos[i+1][1] == 'VERB':\n",
        "            bigram = words_pos[i][0]+' '+words_pos[i+1][0]\n",
        "            bigrams.append(bigram)\n",
        "\n",
        "        if words_pos[i][0] == 'VERB' and words_pos[i+1][1] == 'ADV':\n",
        "            bigram = words_pos[i][0]+' '+words_pos[i+1][0]\n",
        "            bigrams.append(bigram)\n",
        "\n",
        "    return bigrams"
      ],
      "metadata": {
        "id": "o2XyXFS0tCqi"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмем файлы из прошлой домашки, готовые к использованию"
      ],
      "metadata": {
        "id": "pLXZypIOHR0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь создадим 2 множества: в одном будут слова, которые встречаются только в положительных отзывах, а в другом - встречающиеся только в отрицательных."
      ],
      "metadata": {
        "id": "FOT2yiPlHk3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('reviews_train.csv',sep=',', encoding='UTF-8')\n",
        "\n",
        "text_positive = data[data['Sentiment'] == 'Positive']['Text']\n",
        "text_negative = data[data['Sentiment'] == 'Negative']['Text']"
      ],
      "metadata": {
        "id": "sa98-3bmFCiK"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_for_set(text): #функция для словаря отдельная, потому что он нам потом понадобится (спойлер)\n",
        "    all = []\n",
        "    for line in text:\n",
        "        w = line.split(' ')\n",
        "        for word in w:\n",
        "            all.append(word)\n",
        "\n",
        "    d = {}\n",
        "    for item in set(all):\n",
        "        num = all.count(item)\n",
        "        d[item] = num\n",
        "\n",
        "    pure_d = {}\n",
        "    for k, v in d.items():\n",
        "        if v > 10: #здесь те слова, которые встречаются менее 11 раз я убираю, потому что они запутают модель\n",
        "            pure_d[k] = v\n",
        "\n",
        "    return pure_d\n",
        "\n",
        "def set_of_words(text):\n",
        "    d = dict_for_set(text)\n",
        "\n",
        "    return set(d.keys())"
      ],
      "metadata": {
        "id": "1avtcWavHof6"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_pos = set_of_words(text_positive)\n",
        "s_neg = set_of_words(text_negative)"
      ],
      "metadata": {
        "id": "YIX9JpQbHsRh"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим списки с биграммами и словарик со словами для дальнейшего определения тональности"
      ],
      "metadata": {
        "id": "-GrtOEKSiaqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_pos = []\n",
        "bigrams_neg = []\n",
        "\n",
        "for line in text_positive:\n",
        "    bigrams_detect = chunker(line)\n",
        "    for elem in bigrams_detect:\n",
        "        bigrams_pos.append(elem)\n",
        "\n",
        "for line in text_negative:\n",
        "    bigrams_detect = chunker(line)\n",
        "    for elem in bigrams_detect:\n",
        "        bigrams_neg.append(elem)"
      ],
      "metadata": {
        "id": "67MWqj2mIt4j"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_lists = {}\n",
        "freq_lists['Positive'] = dict_for_set(text_positive)\n",
        "freq_lists['Negative'] = dict_for_set(text_negative)"
      ],
      "metadata": {
        "id": "vJs5uOqxHx3k"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовим данные для тестирования выборки"
      ],
      "metadata": {
        "id": "j2R5OMtUinhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = open('reviews_test.csv', encoding='UTF-8')\n",
        "\n",
        "test_dictionary = {}\n",
        "for line in csv.reader(data_test):\n",
        "    test_dictionary[line[0]] = line[1]\n",
        "\n",
        "test_dictionary.pop('Text')\n",
        "\n",
        "data_test.close"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_4-78fWatAr",
        "outputId": "b51e4226-4ca7-4c0b-82ed-8dcadd2884c5"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function TextIOWrapper.close()>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь создадим функцию, которая будет определять, положительный ли отзыв или отрицательный в зависимости от того, какие слова встретились в нём и оценим его accuracy"
      ],
      "metadata": {
        "id": "M4C-oBSiHytz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_sent_detect(freq_lists, text):\n",
        "    counts = Counter()\n",
        "    for sent, freq_list in freq_lists.items():\n",
        "        freq_list = Counter(freq_list)\n",
        "        for word in nltk.word_tokenize(text):\n",
        "            counts[sent] += int(freq_list[word] > 0)\n",
        "    return counts.most_common()"
      ],
      "metadata": {
        "id": "KL6QakilH1AU"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_simple_sent_detect(freq_lists, dictionary):\n",
        "    results = []  # сюда будем писать результаты\n",
        "    gold = []     # сюда будем писать исходную тональность\n",
        "    for line, sent in test_dictionary.items():\n",
        "        predicted_lang = simple_sent_detect(freq_lists, line)[0][0]\n",
        "        results.append(predicted_lang)\n",
        "        gold.append(sent)\n",
        "    print(\"RESULTS:\")\n",
        "    print(\"Accuracy: %.4f\" % accuracy_score(results, gold))"
      ],
      "metadata": {
        "id": "kjqSD0C0bNbW"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_simple_sent_detect(freq_lists, test_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdM296-fbPbZ",
        "outputId": "60e03cb6-4774-4186-cd53-13e9e576639d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS:\n",
            "Accuracy: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь сделаем то же самое, только с биграммами"
      ],
      "metadata": {
        "id": "MFZ3dTgbj4-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_sent_detect_bigrams(bigrams_pos, bigrams_neg, text):\n",
        "\n",
        "    positive = 0\n",
        "    negative = 0\n",
        "\n",
        "    for i in range(0, len(text.split())-1):\n",
        "        big = text.split()[i]+' '+text.split()[i+1]\n",
        "        if big in set(bigrams_pos):\n",
        "            positive+=1\n",
        "        elif big in set(bigrams_neg):\n",
        "            negative+=1\n",
        "\n",
        "    if text.split()[i] in s_pos:\n",
        "        positive+=1\n",
        "    elif text.split()[i] in s_neg:\n",
        "        negative+=1\n",
        "\n",
        "    if positive > negative:\n",
        "        return 'Positive'\n",
        "    elif positive < negative:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Equal'"
      ],
      "metadata": {
        "id": "jIODJOXeRIGv"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_simple_sent_detect_bigrams(bigrams_pos, bigrams_neg,dictionary):\n",
        "    results = []\n",
        "    gold = []\n",
        "\n",
        "    for text,grade in dictionary.items():\n",
        "        pred = simple_sent_detect_bigrams(bigrams_pos, bigrams_neg, text)\n",
        "        results.append(pred)\n",
        "        gold.append(grade)\n",
        "\n",
        "    print(\"RESULTS:\")\n",
        "    print(\"Accuracy: %.4f\" % accuracy_score(results, gold))\n"
      ],
      "metadata": {
        "id": "XpWMdGaxeKZA"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_simple_sent_detect_bigrams(bigrams_pos, bigrams_neg,test_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwNjBeM8fnlM",
        "outputId": "07f78367-c412-40ac-e43e-83c0afe1a250"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS:\n",
            "Accuracy: 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность повысилась на одну десятую, однако у примененного метода есть минусы"
      ],
      "metadata": {
        "id": "FLpbRepxj9iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Количество биграмм с положительной тональностью',len(set(bigrams_pos)),'\\n','Количество биграмм с отрицательной тональностью',len(set(bigrams_neg)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdNSQ5KykOnH",
        "outputId": "a8aac250-58eb-4245-bc27-48a811d6ba88"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество биграмм с положительной тональностью 93 \n",
            " Количество биграмм с отрицательной тональностью 143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как минимум, я не уравнивала количество элементов множеств биграмм с положительной и отрицательной тональностями."
      ],
      "metadata": {
        "id": "9Au2ppRykEUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Более того, выбранные шаблоны можно и нужно оспаривать\n",
        "1. __\"не\"+VERB__\n",
        "\n",
        "      _не могли нарадоваться_ (отметит как отрицательное, а на деле положительное)\n",
        "2. __ADV+VERB__\n",
        "\n",
        "      _хорошо смотрелся бы в борделе_ (отметит как положительное, а на деле отрицательное)\n",
        "3. __VERB+ADV__\n",
        "\n",
        "      _звучало шикарно для глухого_(отметит как положительное, а на деле отрицательное)"
      ],
      "metadata": {
        "id": "97nLWDnSm831"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нет ничего идеального, спасибо за внимание! До новых встреч."
      ],
      "metadata": {
        "id": "9jPaPmlbnqja"
      }
    }
  ]
}